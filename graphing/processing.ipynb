{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83076d11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-09T11:31:04.424749Z",
     "start_time": "2023-10-09T11:31:04.414910Z"
    }
   },
   "source": [
    "# Processing\n",
    "\n",
    "Base version of the notebook that we used to pre-process the collected data, and to then generate CSV files for the graphing. Comments are provided to aid in the usage of the notebook. Modifications may be necessary to get the code functioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abb7ed8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T07:45:55.246424Z",
     "start_time": "2023-09-21T07:45:55.232313Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext jupyternotify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f580372",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-09T11:15:07.663963Z",
     "start_time": "2023-10-09T11:15:06.948121Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import json\n",
    "import datetime\n",
    "from datetime import time, timedelta \n",
    "\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "\n",
    "from scipy import stats\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import csv\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter('ignore', category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b30bf25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T07:45:57.670186Z",
     "start_time": "2023-09-21T07:45:57.662058Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def print_datetime():\n",
    "    \"\"\"Creating a datetime object containing current date and time\"\"\"\n",
    "    \n",
    "    now = datetime.datetime.now()\n",
    "    dt_string = now.strftime('%d/%m/%Y %H:%M:%S')\n",
    "    return dt_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe058a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T07:45:56.407571Z",
     "start_time": "2023-09-21T07:45:56.400840Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setting correct data paths and experiment names\n",
    "\n",
    "base_path = \"/data\" # CHANGE TO CORRECT BASE PATH CONTAINING DATA FOLDERS\n",
    "experiments = ['26.6.2023-scaling-10-results'] # SET TO THE CORRECT RESULTS FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af49ccb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T07:46:36.831131Z",
     "start_time": "2023-09-21T07:46:36.821769Z"
    }
   },
   "outputs": [],
   "source": [
    "service_names = ['primary', 'sift', 'encoding', 'lsh', 'matching']\n",
    "exp_servers = ['gpu', 'cm', 'aws']\n",
    "\n",
    "service_statuses = ['DEPLOY_REQUEST', 'UNDEPLOY_REQUEST', 'DEPLOYED', 'DEAD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5b4397",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T07:46:38.689087Z",
     "start_time": "2023-09-21T07:46:38.652388Z"
    },
    "code_folding": [
     0,
     2,
     6,
     32,
     43,
     52,
     61
    ]
   },
   "outputs": [],
   "source": [
    "# Functions to read the different types of log files \n",
    "\n",
    "def analyse_queue_log(line):\n",
    "    cleaned_line = \"\"\n",
    "    return cleaned_line\n",
    "\n",
    "def read_log(file_name, file_list):\n",
    "    with open(file_name, errors='ignore') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            try:\n",
    "                line = line.replace(\"\\\\\", \"\")\n",
    "                json_line = json.loads(line)\n",
    "                file_list.append(json_line)\n",
    "            except:\n",
    "\n",
    "                try:\n",
    "                    if line == \"\\n\":\n",
    "                        pass\n",
    "                    else:\n",
    "                        ignore_strings = [\"SIFT extraction time =\", \"Incl prefiltering & memcpy =\", \n",
    "                                          \";QUEUE;analytics;{analytics:\", \"Waiting for sidecar service connection...\",\n",
    "                                          \"Sidecar CONNECTED!\", \"Blocking, press ctrl+c to kill the sidecar...\",\n",
    "                                          \"-- Detecting\", \"-- Looking\", \"EOF\", \"%]\", \"warning\", \"Warning\", \"--\",\n",
    "                                          \"Scanning\", \"|\", \"declared here\", \"/home/src/cuda_files.cu:\"]\n",
    "                        res = [partial_string for partial_string in ignore_strings if(partial_string in line)]\n",
    "                        if not bool(res):\n",
    "                            pass\n",
    "                except:\n",
    "                    pass\n",
    "    return file_list\n",
    "\n",
    "def read_eventLogger(file_name, dataframe):\n",
    "    colnames = ['timestamp', 'service', 'event', 'value', '']\n",
    "    events_df = pd.read_csv(file_name.decode(\"utf-8\"), delimiter=';', \n",
    "                            names=colnames)\n",
    "\n",
    "    server_type = file_name.decode('utf-8').split('/')[-1].split('-')[-1].split('.')[0]\n",
    "    events_df['server'] = server_type\n",
    "\n",
    "    dataframe = pd.concat([dataframe, events_df])\n",
    "    return dataframe\n",
    "\n",
    "def read_gpu(file_name, dataframe):\n",
    "    gpu_df = pd.read_csv(file_name.decode('utf-8'), delimiter=',', on_bad_lines='skip')\n",
    "\n",
    "    server_type = file_name.decode('utf-8').split('/')[-1].split('-')[-1].split('.')[0]\n",
    "    gpu_df['server'] = server_type\n",
    "\n",
    "    dataframe = pd.concat([dataframe, gpu_df])\n",
    "    return dataframe\n",
    "\n",
    "def read_gpu_json(file_name, dataframe):\n",
    "    gpu_read_df = pd.read_json(file_name.decode('utf-8'), lines=True)\n",
    "\n",
    "    server_type = file_name.decode('utf-8').split('/')[-1].split('-')[-1].split('.')[0]\n",
    "    gpu_read_df['server'] = server_type\n",
    "\n",
    "    dataframe = pd.concat([dataframe, gpu_read_df])\n",
    "    return dataframe\n",
    "\n",
    "def load_data(experiment_name):\n",
    "    directory = os.fsencode(f'{base_path}/{experiment_name}')  # log directory\n",
    "\n",
    "    # create lists and the dataframes to house the data\n",
    "    client_data = []\n",
    "    pipeline_data = []\n",
    "\n",
    "    events_df = pd.DataFrame()\n",
    "    deployment_df = pd.DataFrame()\n",
    "    gpu_total_df = pd.DataFrame()\n",
    "    gpu_process_df = pd.DataFrame()\n",
    "    gpu_json_df = pd.DataFrame()\n",
    "\n",
    "    for path, subdirs, files in os.walk(directory):\n",
    "        for i in range(len(files)):\n",
    "            curr_file = os.path.join(path, files[i])\n",
    "            file_size = os.path.getsize(curr_file)\n",
    "            print(f\"[{print_datetime()}] Current file is {curr_file.decode('utf-8')} with filesize {file_size/100} KB\")\n",
    "            if file_size == 0:\n",
    "                print(f\"[{print_datetime()}] File {curr_file.decode('utf-8')} has no data, will skip\")\n",
    "                continue\n",
    "            if any(file_ext in curr_file.decode('utf-8')\n",
    "                   for file_ext in ['.sh', '.swp', '.pem']):\n",
    "                pass\n",
    "            elif 'clients.log' in str(curr_file):\n",
    "                read_log(curr_file, client_data)\n",
    "            elif 'pipeline' in str(curr_file):\n",
    "                read_log(curr_file, pipeline_data)\n",
    "            elif 'eventLogger' in str(curr_file):\n",
    "                events_df = read_eventLogger(curr_file, events_df)\n",
    "            elif 'deployment' in str(curr_file):\n",
    "                if 'scal' in experiment_name:\n",
    "                    deployment_file = open(curr_file.decode('utf-8'), \"r\")\n",
    "                    deployment_data = deployment_file.read()\n",
    "                    deployment_data = deployment_data.replace('gpu02;cm-01-05-061', 'gpu02:cm-01-05-061')\n",
    "                    deployment_data_buffer = io.StringIO(deployment_data)\n",
    "                else:\n",
    "                    deployment_data_buffer = curr_file.decode('utf-8')\n",
    "                deployment_df = pd.read_csv(deployment_data_buffer , sep=';')\n",
    "            elif 'gpu' in str(curr_file):\n",
    "                if 'json' in str(curr_file):\n",
    "                    gpu_json_df = read_gpu_json(curr_file, gpu_json_df)\n",
    "                elif 'global' in str(curr_file):\n",
    "                    gpu_total_df = read_gpu(curr_file, gpu_total_df)\n",
    "                elif 'processes' in str(curr_file):\n",
    "                    gpu_process_df = read_gpu(curr_file, gpu_process_df)\n",
    "\n",
    "\n",
    "            print(f\"[{print_datetime()}] Loaded {curr_file.decode('utf-8')} succesfully\")\n",
    "\n",
    "    # preprocessing to be run only once, dataframes are not to be modified after\n",
    "    # convert lists to dataframes if required\n",
    "    client_data = [i for i in client_data if not isinstance(i, int)]\n",
    "    client_df = pd.DataFrame(client_data)\n",
    "\n",
    "    pipeline_clean_ints = [i for i in pipeline_data if not isinstance(i, int)]\n",
    "    pipeline_clean_floats = [i for i in pipeline_clean_ints if not isinstance(i, float)]\n",
    "    pipeline_df = pd.DataFrame(pipeline_clean_floats)\n",
    "\n",
    "    # convert the timestamps to milliseconds\n",
    "    client_df['timestamp'] = client_df['timestamp'].map(lambda x: float(x)/1000)\n",
    "    events_df['timestamp'] = events_df['timestamp'].map(lambda x: float(x)/1000)\n",
    "\n",
    "    # pre-processing for the pipeline df, if timestamp does not have valid data, remove\n",
    "    pipeline_df = pipeline_df.dropna(axis=0, subset=['timestamp'])\n",
    "    pipeline_df['timestamp'] = pipeline_df['timestamp'].astype('string')\n",
    "    pipeline_df = pipeline_df[pipeline_df['timestamp'].str.contains(\"\\d{13}.\\d{2}\", regex=True)]\n",
    "    pipeline_df = pipeline_df[pipeline_df['timestamp'].apply(lambda x: True if len(x) == 16 else False)]\n",
    "    pipeline_df['timestamp'] = pipeline_df['timestamp'].map(lambda x: float(x)/1000)\n",
    "\n",
    "    resources_df = events_df[events_df['event'].isin(['RESOURCES'])]\n",
    "    resources_df['value'] = resources_df['value'].map(lambda x: json.loads(x.replace(\"{log:\\\"\",\"\").replace(\"\\\"}\",\"\").replace(\"null\",\"\\\"\\\"\")))\n",
    "        \n",
    "    dataframes = {\n",
    "        'client': client_df,\n",
    "        'events': events_df,\n",
    "        'pipeline': pipeline_df,\n",
    "        'deployment': deployment_df, \n",
    "        'resources': resources_df,        \n",
    "        'gpu_total': gpu_total_df,\n",
    "        'gpu_process': gpu_process_df,\n",
    "        'gpu_json': gpu_json_df\n",
    "    }\n",
    "    \n",
    "    return dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a739a7b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T07:56:18.074515Z",
     "start_time": "2023-09-21T07:46:39.159976Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the results from the different files as variable raw_results\n",
    "\n",
    "raw_results = {}\n",
    "\n",
    "for experiment in experiments:\n",
    "    print(f'[{print_datetime()}] Current experiment is {experiment}')\n",
    "    raw_results[experiment] = load_data(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9a26a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T07:57:29.853196Z",
     "start_time": "2023-09-21T07:56:28.050928Z"
    },
    "code_folding": [
     0,
     4,
     8,
     27,
     58,
     69
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Pre-processing the raw results\n",
    "\n",
    "preprocessed_results = {}\n",
    "\n",
    "def preprocessing_none(input_df):\n",
    "    \"\"\"Pre-processing happened when data is first loaded, or pre-processing not required\"\"\"\n",
    "    return input_df    \n",
    "\n",
    "def preprocessing_gpu_total_process(input_df):\n",
    "    try:\n",
    "        # rename timestamp to datetime, then convert to UNIX timestamps\n",
    "        df_gpu = input_df.rename(columns={'timestamp': 'datetime'})\n",
    "\n",
    "        # remove rows that do not contain data\n",
    "        df_gpu = df_gpu[df_gpu['datetime'].str.contains('timestamp') == False] \n",
    "\n",
    "        df_gpu['timestamp'] = pd.to_datetime(df_gpu['datetime']) + pd.Timedelta(hours=-1)\n",
    "        df_gpu['timestamp'] = (df_gpu['timestamp'] - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta('1ms') / 1000\n",
    "        df_gpu = df_gpu.sort_values(by='datetime', ascending=True)\n",
    "\n",
    "        # clean column names\n",
    "        df_gpu.columns = df_gpu.columns.str.replace(' ', '').str.replace('\\[.*$', '')\n",
    "        df_gpu = df_gpu.rename(columns=str.lower)\n",
    "    except:\n",
    "        df_gpu = input_df\n",
    "    return df_gpu\n",
    "\n",
    "def preprocessing_gpu_json(input_df):\n",
    "    try:\n",
    "        df_gpu = input_df.rename(columns={'query_time': 'datetime'})\n",
    "        df_gpu = df_gpu.sort_values(by='datetime', ascending=True)\n",
    "        \n",
    "        df_gpu['timestamp'] = pd.to_datetime(df_gpu['datetime']) + pd.Timedelta(hours=-2)\n",
    "        df_gpu['timestamp'] = (df_gpu['timestamp'] - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta('1ms') / 1000\n",
    "\n",
    "        df_time_server = df_gpu[['timestamp', 'datetime', 'server']]\n",
    "\n",
    "        df_gpu['gpus'] = df_gpu['gpus'].apply(lambda x: json.dumps(x))\n",
    "        df_gpu['gpus'] = df_gpu['gpus'].apply(json.loads)\n",
    "        df_gpu = df_gpu.explode(column='gpus').reset_index(drop=True)\n",
    "\n",
    "        df_gpu = pd.concat([df_gpu[['timestamp', 'datetime', 'server']], pd.json_normalize(df_gpu['gpus'])], axis=1)\n",
    "\n",
    "        df_gpu['processes'] = df_gpu['processes'].apply(lambda x: json.dumps(x))\n",
    "        df_gpu['processes'] = df_gpu['processes'].apply(json.loads)\n",
    "        df_gpu = df_gpu.explode(column='processes').reset_index(drop=True)\n",
    "\n",
    "        df_gpu = pd.concat([df_gpu, pd.json_normalize(df_gpu['processes'])], axis=1)\n",
    "        \n",
    "        selected_time = df_gpu[df_gpu['timestamp'].between(1687722739, 1687725650)]\n",
    "        unique_lines = selected_time['processes']\n",
    "        for i in range(len(unique_lines)):\n",
    "            curr_line = unique_lines.iloc[i]['full_command']\n",
    "        \n",
    "    except:\n",
    "        df_gpu = input_df\n",
    "    return df_gpu\n",
    "\n",
    "preprocessing_functions = {\n",
    "    'client': preprocessing_none,\n",
    "    'events': preprocessing_none,\n",
    "    'pipeline': preprocessing_none,\n",
    "    'deployment': preprocessing_none, \n",
    "    'resources': preprocessing_none,        \n",
    "    'gpu_total': preprocessing_gpu_total_process,\n",
    "    'gpu_process': preprocessing_gpu_total_process,\n",
    "    'gpu_json': preprocessing_gpu_json\n",
    "}\n",
    "\n",
    "for experiment, raw_result in raw_results.items():\n",
    "    print(f'Currently pre-processing {experiment}')\n",
    "    preprocessed_results[experiment] = {}\n",
    "    for rr_key, rr_value in raw_result.items():\n",
    "        preprocessed_results[experiment][rr_key] = {}\n",
    "        preprocessed_results[experiment][rr_key] = preprocessing_functions[rr_key](rr_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9ed080",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-09T11:25:21.395043Z",
     "start_time": "2023-10-09T11:25:21.334013Z"
    },
    "code_folding": [
     2,
     52,
     135,
     141,
     179,
     204,
     255,
     288,
     310,
     346,
     456
    ]
   },
   "outputs": [],
   "source": [
    "# Analysis functions\n",
    "\n",
    "def analysis_fps(df_messages_received, deploy_start, num_clients):\n",
    "    \"\"\"Generate the FPS results\"\"\"\n",
    "    \n",
    "    client_count = None\n",
    "    \n",
    "    # find where each client first comes online\n",
    "    fps_data = df_messages_received.sort_values(by=['timestamp'])\n",
    "            \n",
    "    max_timestamp = fps_data['timestamp'].max()\n",
    "    min_timestamp = fps_data['timestamp'].min()\n",
    "\n",
    "    fps_data = fps_data.reset_index()\n",
    "    fps_data = fps_data[fps_data['timestamp'].between(deploy_start, max_timestamp)]\n",
    "    \n",
    "    fps_data[\"frame_no\"] = fps_data[\"frame_no\"].astype(str).astype(int)\n",
    "    \n",
    "    clients = fps_data['id'].unique()[:num_clients+1]\n",
    "    clients_locs = []\n",
    "    for client in clients:\n",
    "        first_occurs = (fps_data['id'] == client).idxmax()\n",
    "        clients_locs.append(first_occurs)\n",
    "    clients_locs.append(len(fps_data))\n",
    "    \n",
    "    try:\n",
    "        rel_data_loc = clients_locs[num_clients+1]\n",
    "        fps_rel_data = fps_data.iloc[:rel_data_loc]\n",
    "        fps_rel_data = fps_rel_data[fps_rel_data.id.isin(clients)]\n",
    "    except:\n",
    "        fps_rel_data = fps_data\n",
    "\n",
    "    client_log_items = fps_rel_data[fps_rel_data['id'].str.contains(clients[-1])]\n",
    "\n",
    "    # find frames closest to frame number 0 and 1600 for curr client\n",
    "    frame_0 = client_log_items.loc[(client_log_items.frame_no - 0).abs().idxmin()]\n",
    "    frame_1600 = client_log_items.loc[(client_log_items.frame_no - 1600).abs().idxmin()]\n",
    "\n",
    "    frames_len = len(client_log_items[client_log_items['frame_no'].between(0, 1600)])\n",
    "    frames_time = frame_1600.timestamp - frame_0.timestamp\n",
    "\n",
    "    fps = frames_len / frames_time\n",
    "    \n",
    "    client_results = {\n",
    "        'results': {\n",
    "            'avg': fps,\n",
    "            'std': 0\n",
    "        }, \n",
    "        'clients': clients\n",
    "    }\n",
    "    return client_results\n",
    "\n",
    "def analyse_metrics(df_client, clients, deploy_start, deploy_end):  \n",
    "    \"\"\"Generating results for E2E latency, jitter, and success rate\"\"\"\n",
    "    \n",
    "    # dictionary to store the results\n",
    "    clients_e2e_results = {}\n",
    "    \n",
    "    pure_e2e_results = []\n",
    "    jitter_results = []\n",
    "    success_rate = []\n",
    "    \n",
    "    latency_values = np.array([])\n",
    "    latency_clients = []\n",
    "    \n",
    "    e2e_latency = 0\n",
    "    \n",
    "    curr_client = clients\n",
    "\n",
    "    df_client_rel = df_client[df_client['timestamp'].between(deploy_start, deploy_end)] # relevant client\n",
    "    client_log_items = df_client_rel[df_client_rel['id'].str.contains(curr_client)].astype('string')\n",
    "    client_log_items['datetime'] = pd.to_datetime(client_log_items['timestamp'], unit='s')\n",
    "\n",
    "    # selecting out the logs related to the application \n",
    "    curr_client_sf = client_log_items[client_log_items['message'].str.contains(\"Sent Frame\")].astype('string')\n",
    "    curr_client_rf = client_log_items[client_log_items['message'].str.contains(\"Received results for\")].astype('string')\n",
    "\n",
    "    # calculating the end-to-end latency\n",
    "    if len(curr_client_sf) > 0:\n",
    "        timestamp_first_frame = curr_client_sf['timestamp'].iloc[0]\n",
    "        total_sent = len(curr_client_sf)\n",
    "        total_recv = len(curr_client_rf)\n",
    "\n",
    "        client_logs_merged = pd.merge(curr_client_sf, curr_client_rf, on='frame_no')\n",
    "        client_logs_merged = client_logs_merged.drop(['service_name_x', 'service_name_y', 'id_y', 'message_x', 'message_y'], axis=1)\n",
    "\n",
    "        client_logs_merged['timestamp_sent'] = pd.to_numeric(client_logs_merged['timestamp_x'])\n",
    "        client_logs_merged['timestamp_recv_results'] = pd.to_numeric(client_logs_merged['timestamp_y'])\n",
    "\n",
    "        client_logs_merged['e2e_latency'] = client_logs_merged['timestamp_recv_results'] - client_logs_merged['timestamp_sent']                    \n",
    "        client_logs_merged['e2e_latency'] = client_logs_merged['e2e_latency'] * 1000 # convert into ms\n",
    "        \n",
    "        latency_values = np.append(latency_values, client_logs_merged['e2e_latency'])\n",
    "        latency_clients.append(client_logs_merged['id_x'].values.tolist())\n",
    "\n",
    "        if total_recv == 0:\n",
    "            e2e_latency = 0\n",
    "        else:\n",
    "            try:\n",
    "                e2e_latency = client_logs_merged['e2e_latency'].median()\n",
    "            except:\n",
    "                e2e_latency = 0\n",
    "\n",
    "        # store the client, time of first frame sent, whether any frames were received,\n",
    "        clients_e2e_results[curr_client] = {\n",
    "            'begin_timestamp': timestamp_first_frame,\n",
    "            'total_sent': total_sent,\n",
    "            'total_recv': total_recv,\n",
    "            'success_rate': (total_recv/total_sent),\n",
    "            'e2e_latency': e2e_latency\n",
    "        }\n",
    "\n",
    "        success_rate.append((total_recv/total_sent)*100)\n",
    "\n",
    "        # calculating the jitter\n",
    "        jitter_subtr = abs(client_logs_merged['e2e_latency'][:-1].to_numpy() - client_logs_merged['e2e_latency'][1:].to_numpy())\n",
    "        jitter_vals = np.sum(jitter_subtr) / len(jitter_subtr)\n",
    "        jitter_results.append(jitter_vals)\n",
    "    \n",
    "    results = {\n",
    "        'e2e': {\n",
    "            'avg': np.nanmedian(e2e_latency), \n",
    "            'std': np.nanstd(e2e_latency)\n",
    "        },\n",
    "        'jitter': {\n",
    "            'avg': np.nanmedian(jitter_results), \n",
    "            'std': np.nanstd(jitter_results)\n",
    "        },\n",
    "        'success_rate': {\n",
    "            'avg': np.nanmedian(success_rate), \n",
    "            'std': np.nanstd(success_rate)\n",
    "        }\n",
    "    }\n",
    "    return results, latency_values, latency_clients\n",
    "\n",
    "def select_value(dictionary, key):\n",
    "    if isinstance(dictionary, list):\n",
    "        return dictionary[0][key]\n",
    "    else:\n",
    "        return dictionary[key]\n",
    "\n",
    "def analysis_cpu_mem(client_resources):\n",
    "    \"\"\"Generating CPU and memory results\"\"\"\n",
    "    \n",
    "    pipeline_resources = client_resources[~client_resources['service'].str.contains('NODE_ENGINE')]\n",
    "    pipeline_services = pipeline_resources['service'].unique()\n",
    "    \n",
    "    pipeline_resource_usage = {\n",
    "        'cpu': {\n",
    "            'avg': 0,\n",
    "            'std': 0\n",
    "        },\n",
    "        'memory': {\n",
    "            'avg': 0,\n",
    "            'std': 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # split gpu_total_resources by server type\n",
    "    server_types = pipeline_resources['server'].unique()\n",
    "    for server in server_types:\n",
    "        curr_server_res = pipeline_resources[pipeline_resources['server'] == server]\n",
    "        for es_server in exp_servers:\n",
    "            if server[:2] == es_server[:2]:\n",
    "                curr_server_services = curr_server_res['service'].unique()                \n",
    "    \n",
    "                # retrieve metric results for each service\n",
    "                for service in curr_server_services:\n",
    "                    service_name = service.split(\".\")[2]\n",
    "                    curr_service_results = pipeline_resources[pipeline_resources['service'].str.contains(service)]\n",
    "                    for rt_key, rt_item in pipeline_resource_usage.items():\n",
    "                        curr_resources = curr_service_results['value'].map(lambda x: select_value(x, rt_key))\n",
    "                        curr_resources = pd.to_numeric(curr_resources)\n",
    "                        \n",
    "                        pipeline_resource_usage[rt_key]['avg'] += curr_resources.median()\n",
    "                        pipeline_resource_usage[rt_key]['std'] += curr_resources.std()     \n",
    "                        \n",
    "    return pipeline_resource_usage\n",
    "\n",
    "def analysis_gpu_total(gpu_total_resources, exp_servers):    \n",
    "    \"\"\"Generating GPU usage results for the total usage of the GPU\"\"\"\n",
    "    \n",
    "    gpu_total_resources = gpu_total_resources[['memory.used', 'server']]\n",
    "    gpu_total_resources['memory'] = gpu_total_resources['memory.used'].str.replace('MiB', '')\n",
    "    gpu_total_resources['memory'] = pd.to_numeric(gpu_total_resources['memory']) * 1.04858 # convert to MB\n",
    "    \n",
    "    gpu_total_memory_avg = 0\n",
    "    gpu_total_memory_std = 0 \n",
    "        \n",
    "    # split gpu_total_resources by server type\n",
    "    server_types = gpu_total_resources['server'].unique()\n",
    "    for server in server_types:\n",
    "        curr_gpu_res = gpu_total_resources[gpu_total_resources['server'] == server]\n",
    "        for es_server in exp_servers:\n",
    "            if server[:2] == es_server[:2]:\n",
    "                gpu_total_memory_avg += curr_gpu_res['memory'].median()\n",
    "                gpu_total_memory_std += curr_gpu_res['memory'].std()\n",
    "    \n",
    "    gpu_resource_results = {\n",
    "        'avg': gpu_total_memory_avg,\n",
    "        'std': gpu_total_memory_std\n",
    "    } \n",
    "    return gpu_resource_results\n",
    "      \n",
    "def analysis_gpu_process(gpu_process_res, exp_servers, df_rel_resource_events):\n",
    "    \"\"\"Generating GPU usage reulsts for each process/service\"\"\"\n",
    "    \n",
    "    gpu_process_resources = gpu_process_res[['used_gpu_memory', 'server', 'timestamp', 'pid']]\n",
    "    gpu_process_resources['memory'] = gpu_process_resources['used_gpu_memory'].str.replace('MiB', '')\n",
    "    gpu_process_resources['memory'] = pd.to_numeric(gpu_process_resources['memory']) * 1.04858 # convert to MB\n",
    "    gpu_process_resources['pid'] = pd.to_numeric(gpu_process_resources['pid'])\n",
    "    \n",
    "    gpu_process_memory_avg = 0\n",
    "    gpu_process_memory_std = 0\n",
    "    \n",
    "    service_results = {}\n",
    "    indiv_process_results = {}\n",
    "\n",
    "    if len(gpu_process_resources) == 0:\n",
    "        return indiv_process_results\n",
    "                \n",
    "    ru_logs_combined = pd.DataFrame() # resource usage logs combined\n",
    "    # compiling all the log messages for the processes on the GPU into one dataframe\n",
    "    for service_type in service_names:\n",
    "        ru_logs_combined = service_process_resources(ru_logs_combined, df_rel_resource_events, service_type)\n",
    "        \n",
    "    # analyse for each service, i.e., selecting all GPU resource usage from each ID\n",
    "    for service in service_names:\n",
    "        curr_service_results = ru_logs_combined[ru_logs_combined['service'] == service]\n",
    "        service_process_ids = curr_service_results['Pid'].unique()\n",
    "                        \n",
    "        for pid in service_process_ids:\n",
    "            curr_service_gpu_results = gpu_process_resources[gpu_process_resources['pid'] == pid]['memory'].values.tolist() \n",
    "            curr_service_gpu_results = pd.to_numeric(curr_service_gpu_results) / 1000\n",
    "                                    \n",
    "            try:\n",
    "                service_results[service] += [curr_service_gpu_results]\n",
    "            except:\n",
    "                service_results[service] = []\n",
    "                service_results[service] += [curr_service_gpu_results]\n",
    "        \n",
    "        # caclulate the average and standard deviation for these processes\n",
    "        if service != \"primary\":\n",
    "            process_gpu_avg = np.nanmedian(service_results[service][0])\n",
    "            process_gpu_std = np.nanstd(service_results[service][0])\n",
    "        elif service == 'primary': \n",
    "            process_gpu_avg = 0\n",
    "            process_gpu_std = 0\n",
    "            \n",
    "        indiv_process_results[service] = {\n",
    "            'avg': process_gpu_avg,\n",
    "            'std': process_gpu_std\n",
    "        }      \n",
    "    return indiv_process_results\n",
    "\n",
    "def analysis_gpu_json(gpu_res, exp_servers):\n",
    "    \"\"\"Generating GPU usage results from the Python GPU utility\"\"\"\n",
    "    service_results = {}\n",
    "\n",
    "    for service in service_names:\n",
    "        try:\n",
    "            curr_service_res = gpu_res[gpu_res['full_command'].apply(lambda x: True if './server' in x[0] else False)]\n",
    "            curr_service_res = curr_service_res[curr_service_res['full_command'].apply(lambda x: True if x[1] == service else False)]\n",
    "            \n",
    "            metrics_to_consider = ['temperature.gpu', 'utilization.gpu', 'memory.used',\n",
    "                                  'gpu_memory_usage', 'cpu_percent', 'cpu_memory_usage']\n",
    "\n",
    "            service_results[service] = {}\n",
    "\n",
    "            for metric in metrics_to_consider: \n",
    "                rel_data = curr_service_res[metric]\n",
    "                rel_data[rel_data == 0] = np.nan\n",
    "                process_avg = np.nanmedian(rel_data)\n",
    "                process_std = np.nanstd(rel_data)\n",
    "\n",
    "                if metric == 'utilization.gpu':\n",
    "                    if process_avg == 0:\n",
    "                        process_avg == process_std\n",
    "\n",
    "                service_results[service][metric] = {}\n",
    "                service_results[service][metric] = {\n",
    "                    'avg': process_avg,\n",
    "                    'std': process_std\n",
    "                }\n",
    "        except:\n",
    "            pass\n",
    "    return service_results\n",
    "\n",
    "def service_process_resources(df_usage, df_input, service_name):    \n",
    "    \"\"\"Generating usage results for the individual services\"\"\"\n",
    "    \n",
    "    rre_data = df_input[df_input['service_name'] == service_name]\n",
    "    rre_data['value'] = rre_data['value'].map(lambda x: x.replace(\"{log:\\\"\",\"\").replace(\"\\\"}\",\"\").replace(\"null\",\"\\\"\\\"\"))\n",
    "    \n",
    "    resource_usage_logs = pd.DataFrame()\n",
    "    resource_usage_logs['server'] = rre_data['server']\n",
    "    resource_usage_logs['timestamp'] = rre_data['timestamp']\n",
    "\n",
    "    resource_usage_logs['value'] = rre_data['value'].apply(json.loads)\n",
    "    resource_usage_logs = resource_usage_logs.explode(column='value').reset_index(drop=True)\n",
    "    resource_usage_logs = pd.concat([resource_usage_logs[['server', 'timestamp']], pd.json_normalize(resource_usage_logs['value'])], axis=1) \n",
    "    \n",
    "    try:\n",
    "        resource_usage_logs['service'] = resource_usage_logs['job_name'].str.extract(\"(?<=percomm.)(.*?)(?=.deploy)\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    df_usage = pd.concat([df_usage, resource_usage_logs])\n",
    "    return df_usage\n",
    "\n",
    "def analysis_cpu_mem_process(df_rel_events, df_rel_resource_events):    \n",
    "    \"\"\"Generating CPU and memory usage results\"\"\"\n",
    "    \n",
    "    ru_logs_combined = pd.DataFrame() # resource usage logs combined\n",
    "    indiv_process_results = {}\n",
    "    \n",
    "    if len(df_rel_resource_events) == 0:\n",
    "        return indiv_process_results\n",
    "    \n",
    "    # compiling all the log messages for the processes into one dataframe\n",
    "    for service_type in service_names:\n",
    "        ru_logs_combined = service_process_resources(ru_logs_combined, df_rel_resource_events, service_type)\n",
    "                \n",
    "    # analyse for each service, i.e., selecting all CPU resource usage from each ID\n",
    "    for service in service_names:\n",
    "        curr_service_results = ru_logs_combined[ru_logs_combined['service'] == service]\n",
    "        indiv_process_results[service] = {}\n",
    "                \n",
    "        cpu_mem = ['cpu', 'memory']\n",
    "        for item in cpu_mem:\n",
    "            curr_results = pd.to_numeric(curr_service_results[item]) \n",
    "            \n",
    "            if item == 'memory':\n",
    "                curr_results = curr_results / 1000 / 1000 / 1000\n",
    "        \n",
    "            process_avg = np.nanmedian(curr_results)\n",
    "            process_std = np.nanstd(curr_results)\n",
    "            \n",
    "            indiv_process_results[service][item] = {}\n",
    "            indiv_process_results[service][item] = {\n",
    "                'avg': process_avg,\n",
    "                'std': process_std\n",
    "            }        \n",
    "            \n",
    "    return indiv_process_results\n",
    "\n",
    "def process_latencies(deploy_start, deploy_end, deployment_clients, df_pipeline):\n",
    "    \"\"\"Generating the processing latency values when a frame is in a service\"\"\"\n",
    "    \n",
    "    pipeline_rel = df_pipeline[df_pipeline['timestamp'].between(deploy_start, deploy_end)]\n",
    "    \n",
    "    services = pipeline_rel['service_name'].unique()\n",
    "\n",
    "    relevant_logs = {\n",
    "        'primary': {\n",
    "            'entry': \"received and has a filesize\",\n",
    "            'exit': \"sent to sift service for processing\"\n",
    "        },\n",
    "        'sift': {\n",
    "            'entry': \"Received data from 'primary' service\",\n",
    "            'exit': \"Sent sift data to lsh\"\n",
    "        },\n",
    "        'encoding': {\n",
    "            'entry': \"sift data received for Frame\",\n",
    "            'exit': \"Forwarded Frame\"\n",
    "        },\n",
    "        'lsh': {\n",
    "            'entry': \"Received data from 'encoding' service\",\n",
    "            'exit': \"Forwarded Frame\"\n",
    "        },\n",
    "        'matching': {\n",
    "            'entry': \"Received data from 'lsh' service and will now\",\n",
    "            'exit': \"sent to client with number of markers of\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    service_results_combined = {}\n",
    "\n",
    "    for service in service_names: \n",
    "        service_latency = np.array([])\n",
    "        for client in deployment_clients: \n",
    "            service_logs = pipeline_rel.loc[pipeline_rel['service_name'] == service]\n",
    "            try:\n",
    "                service_logs = service_logs[service_logs['client_id'].str.contains(client)]\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "            if service == '':\n",
    "                continue\n",
    "            \n",
    "            rl_item = relevant_logs[service]\n",
    "            \n",
    "            # selecting out the logs related to the service \n",
    "            service_entry = service_logs[service_logs['message'].str.contains(rl_item['entry'])].astype('string')\n",
    "            service_exit = service_logs[service_logs['message'].str.contains(rl_item['exit'])].astype('string')\n",
    "            \n",
    "            if \"sift\" == service:\n",
    "                try:\n",
    "                    df_packet_sizes = service_exit['message'].str.extract(\n",
    "                        r'has a service buffer size of (.*?) Bytes and sift buffer size of', \n",
    "                        expand=False)\n",
    "                                        \n",
    "                    df_sift_sizes = service_exit['message'].str.extract(\n",
    "                        r' Bytes and sift buffer size of(.*?) Bytes', \n",
    "                        expand=False)\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            total_entry = len(service_entry)\n",
    "            total_exit = len(service_exit)\n",
    "            if total_exit == 0 or total_entry == 0:\n",
    "                success_rate = 0\n",
    "            else:\n",
    "                success_rate = total_exit/total_entry\n",
    "\n",
    "            service_logs_merged = pd.merge(service_entry, service_exit, on='frame_no')\n",
    "            service_logs_merged = service_logs_merged.drop(\n",
    "                ['service_name_x', 'service_name_y', 'client_id_x', \n",
    "                 'client_id_y',  'message_x', 'message_y'], axis=1)\n",
    "\n",
    "            service_logs_merged['timestamp_entry'] = pd.to_numeric(service_logs_merged['timestamp_x'])\n",
    "            service_logs_merged['timestamp_exit'] = pd.to_numeric(service_logs_merged['timestamp_y'])\n",
    "            \n",
    "            service_logs_merged['latency'] = service_logs_merged['timestamp_exit'] - service_logs_merged['timestamp_entry']            \n",
    "            service_logs_merged['latency'] = service_logs_merged['latency'] * 1000 # convert into ms\n",
    "                        \n",
    "            service_latency = np.append(service_latency, service_logs_merged['latency'].values)\n",
    "                        \n",
    "            service_results = {\n",
    "                'total_entry': total_entry,\n",
    "                'total_exit': total_exit,\n",
    "                'success_rate': success_rate,\n",
    "                'latency': {\n",
    "                    'avg': service_logs_merged['latency'].median(),\n",
    "                    'std': service_logs_merged['latency'].std()\n",
    "                }\n",
    "            }\n",
    "            try:\n",
    "                results[service][client] = service_results\n",
    "            except:\n",
    "                results[service] = {}\n",
    "                results[service][client] = service_results\n",
    "        \n",
    "        latency_avg = np.nanmedian(service_latency)\n",
    "        latency_std = np.nanstd(service_latency)\n",
    "\n",
    "        service_results_combined[service] = {\n",
    "            'avg': latency_avg,\n",
    "            'std': latency_std\n",
    "        }  \n",
    "    \n",
    "    service_results_ordered = {k: service_results_combined[k] for k in \n",
    "                               service_names if k in service_results_combined}\n",
    "    return service_results_ordered \n",
    "\n",
    "def service_statistics(deploy_start, deploy_end, df_rel_events, client_online_times):\n",
    "    \"\"\"Generating service statistics\"\"\"\n",
    "    \n",
    "    deploy_rel_events = df_rel_events[df_rel_events['timestamp'].between(deploy_start, deploy_end)]\n",
    "    service_events = deploy_rel_events[deploy_rel_events['service_name'] != 'client']\n",
    "    \n",
    "    previous_online_time = client_online_times[0] - 120\n",
    "    for i in range(len(client_online_times)):\n",
    "        online_time = client_online_times[i]\n",
    "        \n",
    "        curr_rel_events = service_events[service_events['timestamp'].between(previous_online_time, online_time)] \n",
    "        \n",
    "        previous_online_time = online_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cb69b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T10:31:45.234787Z",
     "start_time": "2023-09-21T10:31:45.151134Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def analyse_deployment(deploy_start, deploy_end, experiment_setup, df_rel_events,\n",
    "                       df_resource_events, df_resources, df_messages_received, \n",
    "                       df_gpu_total, df_gpu_process, df_gpu_json, df_client,\n",
    "                       df_pipeline, curr_client_num):        \n",
    "    \"\"\"Analysing the results for a deployment within a experiment\"\"\"\n",
    "        \n",
    "    # calculating FPS, E2E latency, and jitter\n",
    "    analyse_clients = analysis_fps(df_messages_received, deploy_start, curr_client_num)\n",
    "    fps_results = analyse_clients['results']\n",
    "    deployment_clients = analyse_clients['clients'][-1]\n",
    "            \n",
    "    analysed_metrics = analyse_metrics(df_client, deployment_clients, deploy_start, deploy_end)\n",
    "    metric_calculations = analysed_metrics[0]\n",
    "    deployment_latencies = analysed_metrics[1]\n",
    "    deployment_latencies_clients = analysed_metrics[2]\n",
    "            \n",
    "    # find which servers are being used\n",
    "    exp_servers = set(experiment_setup['permutation'])    \n",
    "    \n",
    "    # obtaining resource usage for current deployment\n",
    "    client_resources = df_resources[df_resources['timestamp'].between(deploy_start, deploy_end)]   \n",
    "    cpu_mem_results = analysis_cpu_mem(client_resources)\n",
    "    \n",
    "    df_resource_events['timestamp'] = pd.to_numeric(df_resource_events['timestamp'])\n",
    "    df_rel_resource_events = df_resource_events[df_resource_events['timestamp'].between(deploy_start, deploy_end)] \n",
    "    \n",
    "    try:\n",
    "        gpu_total_resources = df_gpu_total[df_gpu_total['timestamp'].between(deploy_start, deploy_end)]\n",
    "        gpu_total_results = analysis_gpu_total(gpu_total_resources, exp_servers)\n",
    "    except:\n",
    "        gpu_total_results = 0\n",
    "        \n",
    "    try:\n",
    "        gpu_process_resources = df_gpu_process[df_gpu_process['timestamp'].between(deploy_start, deploy_end)]\n",
    "        gpu_process_results = analysis_gpu_process(gpu_process_resources, exp_servers, df_rel_resource_events)\n",
    "    except:\n",
    "        gpu_process_results = 0\n",
    "        \n",
    "    try:\n",
    "        gpu_resources = df_gpu_json[df_gpu_json['timestamp'].between(deploy_start, deploy_end)]\n",
    "        gpu_results = analysis_gpu_json(gpu_resources, exp_servers)\n",
    "    except:\n",
    "        gpu_results = 0\n",
    "        \n",
    "    # finding latency taken in each process\n",
    "    service_latencies = process_latencies(deploy_start, deploy_end, deployment_clients, df_pipeline)   \n",
    "    \n",
    "    deployment_results = {\n",
    "        'total_period': (deploy_end - deploy_start),\n",
    "        'deploy_start': deploy_start,\n",
    "        'deploy_end': deploy_end,\n",
    "        'fps': fps_results,\n",
    "        'e2e': metric_calculations['e2e'],\n",
    "        'jitter': metric_calculations['jitter'],\n",
    "        'success_rate': metric_calculations['success_rate'],\n",
    "        'cpu': cpu_mem_results['cpu'],\n",
    "        'memory': cpu_mem_results['memory'],\n",
    "        'gpu_total_memory': gpu_total_results,\n",
    "        'gpu_process_memory': gpu_process_results,\n",
    "        'gpu_results': gpu_results,\n",
    "        'service_latencies': service_latencies, \n",
    "        'deployment_latencies': deployment_latencies,\n",
    "        'deployment_clients': deployment_latencies_clients\n",
    "    }\n",
    "    return deployment_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6322a08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-09T11:28:03.039222Z",
     "start_time": "2023-10-09T11:28:03.008440Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def results_to_df(df, results, experiment, experiment_setup):\n",
    "    \"\"\"Generating a dataframe to store the results\"\"\"\n",
    "    \n",
    "    number_clients = len(results)\n",
    "    \n",
    "    for item in results:\n",
    "        curr_client_num = item[0]\n",
    "        curr_client_results = item[1]\n",
    "        \n",
    "        result_row = {\n",
    "            'experiment': experiment, \n",
    "            'permutation': experiment_setup, \n",
    "            'client': curr_client_num,\n",
    "        }\n",
    "        \n",
    "        for curr_result_key, curr_result in curr_client_results.items():\n",
    "            if curr_result_key in ['gpu_total_memory', 'gpu_process_memory', 'gpu_results'] and curr_result == 0:\n",
    "                    continue\n",
    "            if curr_result_key in ['fps', 'e2e', 'jitter', 'success_rate', \n",
    "                                   'cpu', 'memory', 'gpu_total_memory']:\n",
    "                for crk_key, crk_val in curr_result.items():\n",
    "                    result_row[f'{curr_result_key}_{crk_key}'] = crk_val\n",
    "            elif curr_result_key in ['gpu_results', 'cpu_mem']:\n",
    "                for service, crk_val in curr_result.items():\n",
    "                    for scr_key, crkv_v in crk_val.items():\n",
    "                        for crkvv_k, crkvv_v in crkv_v.items():\n",
    "                            if 'gpu' in curr_result_key:\n",
    "                                result_row[f'gpu_{service}_{scr_key}_{crkvv_k}'] = crkvv_v\n",
    "                            else:\n",
    "                                result_row[f'{scr_key}_{service}_{crkvv_k}'] = crkvv_v\n",
    "            elif curr_result_key in ['service_latencies', 'gpu_process_memory']:\n",
    "                for service, crk_val in curr_result.items():\n",
    "                    for scr_key, crkv_v in crk_val.items():\n",
    "                        if 'gpu' in curr_result_key:\n",
    "                            result_row[f'gpu_{service}_{scr_key}'] = crkv_v\n",
    "                        else:\n",
    "                            result_row[f'services_{service}_{scr_key}'] = crkv_v\n",
    "            else:\n",
    "                result_row[curr_result_key] = curr_result\n",
    "        df = df.append(result_row, ignore_index=True)\n",
    "    return df \n",
    "\n",
    "def analyse_experiment(df_combined_results, experiment, experiment_start, \n",
    "                       experiment_end, experiment_setup, df_deployment_events, \n",
    "                       df_rel_events, df_resource_events, df_resources,\n",
    "                       df_messages_received, df_gpu_total, df_gpu_process, \n",
    "                       df_gpu_json, df_client, df_pipeline):\n",
    "    \"\"\"Analysing one experiment/permutation which includes different permutations\"\"\"\n",
    "    \n",
    "    df_deployment_events['timestamp'] = df_deployment_events['timestamp'].astype(str).astype(int)\n",
    "    df_rel_deploy = df_deployment_events[df_deployment_events['timestamp'].between(experiment_start, experiment_end)]\n",
    "\n",
    "    deploy_start = None\n",
    "    deploy_end = None\n",
    "    curr_experiment_setup = None\n",
    "    \n",
    "    last_msg_time = \"\"\n",
    "\n",
    "    results = [] # list to store results from analyse_deployment function \n",
    "    clients = {} # dictionary to store clients and their initialisation time\n",
    "\n",
    "    deployments = [] # list to store deployments\n",
    "\n",
    "    curr_deployment = {}\n",
    "    \n",
    "    client_up = []\n",
    "\n",
    "    # find each deployment\n",
    "    for i, event in df_rel_deploy.iterrows():      \n",
    "        if event['event'] == 'DEPLOY_START':\n",
    "            curr_deployment = {}\n",
    "            deploy_start = float(event['timestamp'])\n",
    "            curr_experiment_setup = json.loads(event['value'].replace(\"\\'\",\"\\\"\"))\n",
    "            \n",
    "            client_up = []\n",
    "\n",
    "            curr_deployment['deploy_start'] = deploy_start\n",
    "        elif event['event'] == 'CLIENT_UP':\n",
    "            num_clients = json.loads(event['value'].replace(\"\\'\",\"\\\"\"))['clients']\n",
    "            client_up_time = event['timestamp']\n",
    "        \n",
    "            curr_deployment[f'client{num_clients}'] = client_up_time\n",
    "            client_up.append(int(event['timestamp']))\n",
    "\n",
    "        elif event['event'] == 'UNDEPLOY':\n",
    "            deploy_end = float(event['timestamp'])\n",
    "            \n",
    "            curr_deployment['deploy_end'] = deploy_end\n",
    "            deployments.append(curr_deployment)\n",
    "            \n",
    "    total_deployments = len(deployments)\n",
    "    for i in range(total_deployments):\n",
    "        curr_deployment = deployments[i]\n",
    "\n",
    "        deployment_begin = curr_deployment['deploy_start']\n",
    "\n",
    "        if i+1 == total_deployments:\n",
    "            deployment_end = experiment_end\n",
    "        else:\n",
    "            deployment_end = deployments[i+1]['deploy_start']\n",
    "        print(f\"Deployment began at {pd.to_datetime(deployment_begin, unit='s')} ({deployment_begin}) and ended at {pd.to_datetime(deployment_end, unit='s')} ({deployment_end}) with setup {curr_experiment_setup}\")\n",
    "\n",
    "        df_rel_msgs_rcvd = df_messages_received[df_messages_received['timestamp'].between(deployment_begin, deployment_end)]\n",
    "           \n",
    "        client_online_times = []\n",
    "        client_ids = df_rel_msgs_rcvd['id'].unique()\n",
    "        clients = [i for i in curr_deployment if i.startswith('client')]\n",
    "                \n",
    "        # for each client find the first message in client_df\n",
    "        for j in range(len(client_ids)):\n",
    "            curr_client = client_ids[j]\n",
    "\n",
    "            df_client_rel = df_client[df_client['timestamp'].between(deployment_begin, deployment_end)]\n",
    "            client_first_msg = df_client_rel[df_client_rel['id'] == curr_client].iloc[0]\n",
    "            client_online_times.append(client_first_msg['timestamp'])\n",
    "            \n",
    "        print(f\"Clients {client_ids} came online at {client_online_times}\")\n",
    "        \n",
    "        # analysing service statistics\n",
    "        service_statistics(experiment_start, experiment_end, df_rel_events, client_online_times)\n",
    "        \n",
    "        num_clients = len(clients)\n",
    "\n",
    "        if num_clients > 0:\n",
    "            for j in range(num_clients):\n",
    "                curr_client_num = j\n",
    "                deploy_start = curr_deployment['client0']\n",
    "                \n",
    "                if curr_client_num+1 == num_clients:\n",
    "                    deploy_end = deployment_end #curr_deployment['deploy_end']\n",
    "                else:\n",
    "                    try:\n",
    "                        deploy_end = curr_deployment[f'client{curr_client_num+1}']\n",
    "                        if deploy_end > deployment_end:\n",
    "                            deploy_end = deployment_end #curr_deployment['deploy_end']\n",
    "                    except:\n",
    "                        deploy_end = deployment_end #curr_deployment['deploy_end']\n",
    "            \n",
    "                temp_rel_msgs_rcvd = df_messages_received[df_messages_received['timestamp'].between(deploy_start, deploy_end)]\n",
    "                deployment_results = analyse_deployment(\n",
    "                    deploy_start, deploy_end, experiment_setup, df_rel_events,\n",
    "                    df_resource_events, df_resources, df_rel_msgs_rcvd, \n",
    "                    df_gpu_total, df_gpu_process, df_gpu_json, df_client,\n",
    "                    df_pipeline, curr_client_num, \n",
    "                )\n",
    "                results.append([j, deployment_results])\n",
    "                \n",
    "        # convert results for this current permutation into a dataframe\n",
    "        df_combined_results = results_to_df(df_combined_results, results, experiment, experiment_setup)\n",
    "    return df_combined_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0739c696",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T10:45:45.386048Z",
     "start_time": "2023-09-21T10:31:46.637481Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Looping through each experiment and analysing their results\n",
    "\n",
    "for experiment, pp_results in preprocessed_results.items():\n",
    "    print(f'[{print_datetime()}] Current experiment to analyse is {experiment}')\n",
    "    experiment_results = pp_results\n",
    "    \n",
    "    # indexing out the pre-processed data\n",
    "    df_deployments = experiment_results['deployment']\n",
    "    df_events = experiment_results['events']\n",
    "    df_gpu_total = experiment_results['gpu_total']\n",
    "    df_gpu_process = experiment_results['gpu_process']\n",
    "    df_gpu_json = experiment_results['gpu_json']\n",
    "    df_resources = experiment_results['resources']\n",
    "    df_client = experiment_results['client']\n",
    "    df_pipeline = experiment_results['pipeline']\n",
    "        \n",
    "    # select out the deployment events for this experiment\n",
    "    df_deployment_events = df_deployments[df_deployments['event'].isin(\n",
    "        ['DEPLOY_START','UNDEPLOY', 'EXPERIMENT_START', 'EXPERIMENT_END', 'CLIENT_UP']\n",
    "    )] \n",
    "        \n",
    "    ###\n",
    "    \n",
    "    # select out the events in question from the 'service' column that match any of the above statuses\n",
    "    df_rel_events = df_events[df_events['event'].isin(service_statuses)]\n",
    "\n",
    "    # create new column of just the pure service name, and of the number of the \n",
    "    df_rel_events['service_name'] = df_rel_events['service'].str.extract(\"(?<=percomm.)(.*?)(?=.deploy)\")\n",
    "    df_rel_events['service_number'] = df_rel_events['service'].str.extract(\".(?<=instance.)(.*?)$\")\n",
    "\n",
    "    # make new timestamp column that's in seconds, and in datetime for printability\n",
    "    df_rel_events['timestamp_secs'] = df_rel_events['timestamp']\n",
    "    df_rel_events['datetime'] = pd.to_datetime(df_rel_events['timestamp_secs'], unit='s')\n",
    "        \n",
    "    ###\n",
    "    \n",
    "    # select out the resource logging\n",
    "    df_resource_events = df_events[df_events['event'].str.contains('RESOURCES')].astype('string')\n",
    "    df_resource_events = df_resource_events[df_resource_events['service'] != 'NODE_ENGINE']\n",
    "    df_resource_events['service_name'] = df_resource_events['service'].str.extract(\"(?<=percomm.)(.*?)(?=.deploy)\")\n",
    "\n",
    "    df_messages_received = df_client[df_client.message.str.contains('Received')]\n",
    "    df_messages_received['datetime'] = pd.to_datetime(df_messages_received['timestamp'], unit='s')\n",
    "    print(f\"Client logs between {df_messages_received['datetime'].min()} and ended at {df_messages_received['datetime'].max()}\\n\")\n",
    "    \n",
    "    df_messages_sent = df_client[df_client.message.str.contains('Sent')]\n",
    "    df_messages_sent['datetime'] = pd.to_datetime(df_messages_sent['timestamp'], unit='s')\n",
    "    \n",
    "    ###\n",
    "    \n",
    "    # iterate through all the experiments\n",
    "    experiment_count = 0 # keep track of the experiments\n",
    "    \n",
    "    experiment_setup = None\n",
    "    experiment_start = None\n",
    "    experiment_end = None\n",
    "            \n",
    "    df_combined_results = pd.DataFrame() # store the results\n",
    "    for i, event in df_deployment_events.iterrows():\n",
    "        if event['event'] == 'EXPERIMENT_START':\n",
    "            experiment_start = float(event['timestamp'])\n",
    "            experiment_setup = json.loads(event[\"value\"].replace(\"\\'\",\"\\\"\"))\n",
    "            continue\n",
    "        if event['event'] == 'EXPERIMENT_END':\n",
    "            experiment_end = float(event['timestamp'])\n",
    "            \n",
    "            # check if there is both an experiment start and end time that are sequential, if not do no analysis \n",
    "            if (experiment_end-experiment_start) > 0:\n",
    "                print(f\"Experiment began at {pd.to_datetime(experiment_start, unit='s')} ({experiment_start}) and ended at {pd.to_datetime(experiment_end, unit='s')} ({experiment_end})\")\n",
    "                print(f\"The experiment had the following setup/permutation:\\n{str(experiment_setup)}\")\n",
    "                \n",
    "                df_combined_results = analyse_experiment(df_combined_results, experiment,\n",
    "                    experiment_start, experiment_end, experiment_setup, \n",
    "                    df_deployment_events, df_rel_events, df_resource_events, \n",
    "                    df_resources, df_messages_received, df_gpu_total, \n",
    "                    df_gpu_process, df_gpu_json, df_client, df_pipeline)\n",
    "                print(\"\\n\")\n",
    "                experiment_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0980476b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
